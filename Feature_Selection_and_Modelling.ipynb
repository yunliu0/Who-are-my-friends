{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.metrics import roc_auc_score, precision_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Train_10000.txt','rb') as file:\n",
    "    dataframe_train=pickle.load(file)\n",
    "train, test = train_test_split(dataframe_train, test_size=0.2, random_state=10)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Distribution of Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "LI=[]\n",
    "for name in dataframe_train.columns:\n",
    "        fig, ax = plt.subplots(figsize=(5, 4))\n",
    "        for group in [0,1]:\n",
    "            try:\n",
    "                sns.distplot(dataframe_train.loc[dataframe_train.Label == group, name],kde=True, label=group)\n",
    "            except:\n",
    "                LI.append(name)  \n",
    "        ax.set_ylabel('Total Count')\n",
    "        ax.set_title(name)\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete High Corrlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defore deletion\n",
    "cor= dataframe_train.corr(method='pearson')\n",
    "\n",
    "fig, ax =plt.subplots(figsize=(8, 10))\n",
    "plt.title(\"Correlation Plot\")\n",
    "sns.heatmap(cor, mask=np.zeros_like(cor, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleted features for Train data\n",
    "dels=list(dataframe_train.iloc[:,6:-2].columns)+['Shortest_path']#,'Source_following','Sink_following','HD','AAI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train set\n",
    "X_train = train.drop(dels,axis=1)\n",
    "y_train = train.Label\n",
    "\n",
    "# test set\n",
    "X_test = test.drop(dels,axis=1)\n",
    "y_test = test.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation after Deletion\n",
    "cor= X_train.corr(method='pearson')\n",
    "fig, ax =plt.subplots(figsize=(8, 10))\n",
    "plt.title(\"Correlation Plot\")\n",
    "sns.heatmap(cor, mask=np.zeros_like(cor, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "statictial selection\n",
    "https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2,f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_= 'all' # control the number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = SelectKBest(score_func=chi2, k=k_)\n",
    "fs.fit(X_train,y_train)\n",
    "\n",
    "col=list(X_train.columns)\n",
    "# the score for all features\n",
    "for i in range(len(fs.scores_)):\n",
    "    print('%s: %3f' % (col[i], fs.scores_[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = fs.transform(X_train)\n",
    "\n",
    "x_test = fs.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_model = GaussianNB()\n",
    "gnb_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelnb = gnb_model.predict(x_test)\n",
    "probnb = gnb_model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=roc_auc_score(y_test,labelnb)\n",
    "ac1=precision_score(y_test,labelnb)\n",
    "print('The ROC of Naive Bayes: %5.3f' %(nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_model = LogisticRegression(penalty ='none',max_iter=500)\n",
    "LR_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predict=LR_model.predict(x_test)\n",
    "prob_preidct=LR_model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl=roc_auc_score(y_test,label_predict)\n",
    "ac2=precision_score(y_test,label_predict)\n",
    "print('The ROC of Logistic Regression: %5.3f' %(rl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt_model=DecisionTreeClassifier()\n",
    "\n",
    "# Random Forest\n",
    "rf_model=RandomForestClassifier(criterion='entropy',n_estimators=200,max_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "dt_model.fit(x_train, y_train)\n",
    "\n",
    "# Random Forest\n",
    "rf_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "labeldt=dt_model.predict(x_test)\n",
    "probdt = dt_model.predict_proba(x_test)\n",
    "\n",
    "# Random Forest\n",
    "labelrf = rf_model.predict(x_test)\n",
    "probrf = rf_model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=roc_auc_score(y_test,labeldt)\n",
    "ac3=precision_score(y_test,labeldt)\n",
    "print('The ROC of Decision Tree: %5.3f' %(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=roc_auc_score(y_test,labelrf)\n",
    "ac4=precision_score(y_test,labelrf)\n",
    "print('The ROC of Random Forest: %5.3f' %(rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelknn = knn.predict(x_test)\n",
    "probrknn = knn.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn=roc_auc_score(y_test,labelknn)\n",
    "ac5=precision_score(y_test,labelknn)\n",
    "print('The ROC of KNN: %5.3f' %(kn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SGDClassifier with Loss='log' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDcl=SGDClassifier(loss='log',max_iter=1000, tol=1e-3)  \n",
    "SGDcl.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predictC=SGDcl.predict(x_test)\n",
    "prob_preidctC=SGDcl.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl=roc_auc_score(y_test,label_predictC)\n",
    "ac6=precision_score(y_test,label_predictC)\n",
    "print('The ROC of SGDClassifier: %5.3f' %(cl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC,SVR\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "estimator = SVR(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train=selector.transform(X_train)\n",
    "SVM_model = SVC(gamma='scale',probability=True)\n",
    "SVM_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsvm = SVM_model.predict(x_test)\n",
    "probrsvm = SVM_model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm=roc_auc_score(y_test,labelsvm)\n",
    "ac7=precision_score(y_test,labelsvm)\n",
    "print('The ROC of SVM: %5.3f' %(svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(10),activation='logistic', random_state=1,max_iter=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probmlp=mlp_model.predict_proba(x_test)\n",
    "labelmlp=mlp_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp=roc_auc_score(y_test,labelmlp)\n",
    "ac8=precision_score(y_test,labelmlp)\n",
    "print('The ROC of MLP: %5.3f' %(mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. RFE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE\n",
    "estimator = SVR(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFE_model = RFE(estimator, n_features_to_select=k_, step=1)\n",
    "RFE_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the label prediction\n",
    "labelrfe = RFE_model.predict(x_test)\n",
    "l=list(labelrfe)\n",
    "ll=[]\n",
    "for elem in l:\n",
    "    if elem <=0.5:\n",
    "        ll.append(0)\n",
    "    else:\n",
    "        ll.append(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe=roc_auc_score(list(y_test),ll)\n",
    "ac9=precision_score(list(y_test),ll)\n",
    "print('The ROC score: %5.3f' %(rfe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = [nb,rl,dt,rf,kn,cl,svm,mlp,rfe]\n",
    "precision=[ac1,ac2,ac3,ac4,ac5,ac6,ac7,ac8,ac9]\n",
    "index1= ['Naive Bayes','Logistic Regression','Decision Tree','Random Forest','K-Nearest Neighbor','SGDClassifier','SVM','Multilayer Perceptron','Recursive Feature Elimination']\n",
    "result = pd.DataFrame(score,index=index1, columns=['ROC'])\n",
    "result['Precision'] = precision\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling the Given Dataset with Probability\n",
    "\n",
    "Untill now **Random Forest** gives the higtest ROC score 0.7622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Test_22.txt','rb') as file:\n",
    "    test=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleted features\n",
    "delt=list(test.iloc[:,6:-2].columns)+['Shortest_path']\n",
    "test=test.drop(delt,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = fs.transform(test)\n",
    "Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ID.txt','rb') as file:\n",
    "    ID=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(probs, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write('Id,Predicted\\n')\n",
    "        for i, p in zip(ID, probs):\n",
    "            file.write(\"{},{}\\n\".format(i, p[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission1(probs, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write('Id,Predicted\\n')\n",
    "        for i, p in zip(ID, probs):\n",
    "            file.write(\"{},{}\\n\".format(i, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the models without scaler transformed\n",
    "clf1 = [LR_model,dt_model,rf_model,knn,gnb_model]\n",
    "prob1=[]\n",
    "for clf in clf1:\n",
    "    prob=clf.predict_proba(Test)\n",
    "    prob1.append(prob)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prob=LR_model.predict_proba(Test)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(prob1[0], 'LR_submit.csv')\n",
    "submission(prob1[1], 'DT_submit.csv')\n",
    "submission(prob1[2], 'RF_submit.csv')\n",
    "submission(prob1[3], 'KNN_submit.csv')\n",
    "submission(prob1[4], 'NB_submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the models with scaler transformed\n",
    "clf2=[SGDcl,SVM_model,mlp_model]\n",
    "Test_=scaler.transform(Test)\n",
    "prob2=[]\n",
    "for clf in clf2:\n",
    "    prob=clf.predict_proba(Test_)\n",
    "    prob2.append(list(prob))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prob=mlp_model.predict_proba(Test_)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission(prob2[0], 'SGD_submit.csv')\n",
    "submission(prob2[1], 'SVM_submit.csv')\n",
    "submission(prob2[2], 'MLP_submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission1(probrfe, 'RFE_submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs=['NB_submit.csv','LR_submit.csv','DT_submit.csv','RF_submit.csv','KNN_submit.csv','SGD_submit.csv','SVM_submit.csv','MLP_submit.csv','RFE_submit.csv']\n",
    "P=[]\n",
    "N=[]\n",
    "for csv in csvs:\n",
    "    dd=pd.read_csv(csv)\n",
    "    l=list(dd.Predicted)\n",
    "    z=[]\n",
    "    o=[]\n",
    "    for elem in l:\n",
    "        if elem <=0.5:\n",
    "            z.append(elem)\n",
    "        else:\n",
    "            o.append(elem)\n",
    "    P.append(len(o))\n",
    "    N.append(len(z))\n",
    "result['Positive'] = P\n",
    "result['Negative'] = N\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Test on Different Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_5850.txt','rb') as file:\n",
    "    self_test=pickle.load(file)\n",
    "self_test_sample=self_test.sample(n = 2000, random_state = 1)\n",
    "self_test_sample=self_test_sample.drop(delt,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_test_sample_label=self_test_sample.Label\n",
    "self_test_sample=self_test_sample.drop('Label',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_test_sample = fs.transform(self_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=[rf_model,LR_model,dt_model,knn,rf_model,SGDcl,SVM_model,mlp_model]\n",
    "one=[]\n",
    "two=[]\n",
    "for mod in model:\n",
    "    labelrf_self = mod.predict(self_test_sample)\n",
    "    probrf_self = mod.predict_proba(self_test_sample)\n",
    "    rf_self=roc_auc_score(self_test_sample_label,labelrf_self)\n",
    "    ac4_self=precision_score(self_test_sample_label,labelrf_self)\n",
    "    one.append(rf_self)\n",
    "    two.append(ac4_self)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rult=pd.DataFrame(index=index1)\n",
    "rult['ROC_Train']=score\n",
    "rult['ROC_Test']=one\n",
    "rult['Precision_Train']=precision\n",
    "rult['Precision_Test']=two\n",
    "rult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
